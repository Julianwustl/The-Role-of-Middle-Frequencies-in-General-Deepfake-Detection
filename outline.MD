## Understanding Deepfake Detection through Foundation Models and Frequency Analysis

### **Research Aim**

Our main objective is to explore effective ways to detect deepfakes. While many methods exist in the literature, a significant proportion center around training specific models to identify particular types of deepfake alterations, as highlighted in [3].

### **Challenges in Current Approaches**

Research indicates that models trained on a single dataset often perform poorly when exposed to data from a different distribution, a phenomenon termed "Domain Shift". A prevalent reason for this is models tend to overfit to specific data, as discussed in [4]. Current efforts are leaning towards either employing advanced custom models or leveraging large foundation models for detection [2]. Notably, in foundation models, encoder weights remain constant, and only a basic linear layer undergoes training.

### **Personal Observations**

My own experiments echo these findings. I compared the performance of a basic linear layer to an MLP layer, utilizing the embedding space (specifically the CLS token) of transformer networks. I employed two encoders: CLIP [5] and DINO [6]. CLIP undergoes training in a multimodal contrastive manner, wherein image captions are mapped to the same embedding space as the image, minimizing their cosine distance. DINO, conversely, trains exclusively on unlabeled image data, adopting a teacher-student structure.

Interestingly, CLIP consistently outperformed DINO by a margin of 5-10% in F1 accuracy. This raises a pivotal question: Why does CLIP, despite DINO having access to more data and being larger, demonstrate superior performance?

### **Analysis of Synthetic Image Characteristics**

In a study [1], there's an analysis of the Fourier and autocorrelation of images generated by different models, such as Diffusion and GAN-based architectures. The research interestingly reveals unique features in fake images, distinguishing them from genuine ones. These characteristics, as the paper suggests, could be harnessed for deepfake detection.

Subsequently, [2] reveals that foundation models excel in distinguishing fake from real images. The pressing question then becomes: Do these foundation models discern fake images by identifying the unique features unveiled in [1]? To resolve this query, it's imperative to demonstrate (or even statistically validate) that these unique features are indeed recognized and processed by foundation models.

### **Proposed Methodologies**

1. **Frequency Band Manipulation**: One tactic involves eliminating specific frequency bands from images. By doing this, we can examine how this impacts classifier performance or analytically juxtapose the embedding space of the original image with that of the modified one. The technique entails employing the 2D FFT, identifying and nullifying spikes, followed by an inverse transform. As per [7], erasing certain frequency bands influences performance, suggesting that the ViT might indeed capture some of this information.

2. **Analyzing Image Features in Latent Space**: We aim to determine, either mathematically or statistically, if the attributes of the original image are detectable in the latent space or the embedding space.

3. **Probing Task for Frequency Band Prediction**: Another approach involves a probing task, attempting to forecast the predominant frequency bands of an image. This could entail training a classifier to select from a set of frequency bands or devising a generative model to predict the frequency band, though the feasibility of the latter remains uncertain.

### **Significance of the Research**

- **Relevance of Distinctive Properties**: If we can validate that the unique features from [1] are indeed present in the latent or embedding space, it would corroborate the idea that linear classifiers primarily harness these features for deepfake detection. While [2] emphasizes that low-level features significantly influence model classification performance, it remains unclear which exact features are employed.

- **Frequency Information's Role**: If we can confirm that frequency data is absent, it would indicate that foundation models depend on alternate information, guiding researchers towards novel detection avenues.

### **References**

- [1] [Properties of Diffusion and Gan Models](https://openaccess.thecvf.com/content/CVPR2023W/WMF/papers/Corvi_Intriguing_Properties_of_Synthetic_Images_From_Generative_Adversarial_Networks_to_CVPRW_2023_paper.pdf)
- [2] [Clip Embedding for Deepfake Detection](https://arxiv.org/pdf/2304.00500.pdf)
- [3] [FaceForensics ++](https://arxiv.org/pdf/1901.08971v3.pdf)
- [4] [Identity Leakage](https://arxiv.org/pdf/2210.14457.pdf)
- [5] [CLIP](https://arxiv.org/abs/2103.00020)
- [6] [DINO](https://arxiv.org/abs/2104.14294)
- [7] [Frequency Analysis](https://arxiv.org/pdf/2204.00993.pdf)

---

This revised version presents the research idea and questions in a structured and clearer manner, making it more accessible to readers.